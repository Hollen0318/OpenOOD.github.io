<!DOCTYPE html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-178132094-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "UA-178132094-1");
  </script>

  <meta charset="UTF-8" />
  <!--  <meta name="viewport" content="width=device-width, initial-scale=1" />-->
  <meta name="viewport" content="width=1024" />
  <title>RobustBench: Adversarial robustness benchmark</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/foundation/6.4.3/css/foundation.min.css" />
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" />
  <script src="https://kit.fontawesome.com/b939870cfb.js" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/foundation/6.4.3/css/foundation.min.css">
  <link rel="stylesheet" href="https://cdn.datatables.net/1.10.24/css/dataTables.foundation.min.css">
  <script type="text/javascript" src="https://code.jquery.com/jquery-3.5.1.js"></script>
  <script type="text/javascript" src="https://cdn.datatables.net/1.10.24/js/jquery.dataTables.min.js"></script>
  <script type="text/javascript" src="https://cdn.datatables.net/1.10.24/js/dataTables.foundation.min.js"></script>

  <link rel="stylesheet" href="./css/main.css" />
</head>


<body>
  <nav class="navbar navbar-expand-md">
      <div class="container">
        <a class="navbar-brand" href="./index.html"
          >RobustBench</a>
        <button
          class="navbar-toggler navbar-light"
          type="button"
          data-toggle="collapse"
          data-target="#main-navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="main-navigation">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link" href="#leaderboard">Leaderboards</a>
            </li>
            <li>
              <a class="nav-link" href="https://arxiv.org/abs/2010.09670">Paper</a>
            </li>
            <li>
              <a class="nav-link" href="#faq"
                >FAQ</a
              >
            </li>
            <li>
              <a class="nav-link" href="#contribute">Contribute</a>
            </li>
            <li>
              <a class="nav-link text-nowrap" href="https://github.com/RobustBench/robustbench"
                >Model Zoo ðŸš€</a 
              >
            </li>
          </ul>
        </div>
      </div>
    </nav>


  <!-- <hr class="toprule" /> -->
  <header>
    <div class="header-block container">
      <div class="logo"><img src="./images/logo.png" alt="logo" /></div>
      <div class="title">RobustBench</div>
      <div class="description">
        A standardized benchmark for adversarial robustness
      </div>
    </div>
  </header>
  <!-- <hr class="toprule" /> -->

  <div class="container">
    <section id="introduction">
      <div class="overview">
        <p class="doublealign">
          The goal of <strong>RobustBench</strong> is to systematically track
          the <em>real</em> progress in adversarial robustness. There are
          already
          <a href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html">more than 3&#39;000
            papers</a>
          on this topic, but it is still unclear which approaches really work
          and which only lead to
          <a href="https://arxiv.org/abs/1802.00420">overestimated robustness</a>. We start from benchmarking common
          corruptions,
          \(\ell_\infty\)- and
          \(\ell_2\)-robustness since these are the most studied settings in the
          literature. We use
          <a href="https://github.com/fra31/auto-attack">AutoAttack</a>, an
          ensemble of white-box and black-box attacks, to standardize the
          evaluation (for details see <a href="https://arxiv.org/abs/2010.09670">our paper</a>) of the \(\ell_p\)
          robustness and
          CIFAR-10-C for the evaluation of robustness to common corruptions. Additionally,
          we open source the
          <a href="https://github.com/RobustBench/robustbench">RobustBench library</a>
          that contains models used for the leaderboard to facilitate their
          usage for downstream applications. <br><br>

          To prevent potential overadaptation of new defenses to AutoAttack, we also welcome external evaluations based
          on <em>adaptive attacks</em>, especially where AutoAttack <a href="https://github.com/fra31/auto-attack/blob/master/flags_doc.md">flags</a>
          a potential overestimation of robustness. For each model, we are interested in the best known robust accuracy
          and see AutoAttack and adaptive attacks as complementary.

          <br><br>
          <strong> News:</strong>
          <ul>
            <li>  <strong>May 2022:</strong> 
              We have extended the common corruptions leaderboard on ImageNet with <a href="https://3dcommoncorruptions.epfl.ch">3D Common Corruptions</a> (ImageNet-3DCC). ImageNet-3DCC evaluation is interesting since (1) it includes more realistic corruptions and (2) it can be used to assess generalization of the existing models which may have overfitted to ImageNet-C. For a quickstart, click <a href="https://github.com/RobustBench/robustbench#new-evaluating-robustness-of-imagenet-models-against-3d-common-corruptions-imagenet-3dcc">here</a>. See the new leaderboard with ImageNet-C and ImageNet-3DCC <a href="https://robustbench.github.io/#div_imagenet_corruptions_heading">here</a> (also mCE metrics can be found <a href="https://github.com/RobustBench/robustbench#corruptions-imagenet-c--imagenet-3dcc">here</a>).
             </li>
             <li>  <strong>May 2022:</strong> 
              We fixed the preprocessing issue for ImageNet corruption evaluations: previously we used resize to 256x256 and central crop to 224x224 which wasn't necessary since the ImageNet-C images are already 224x224. Note that this changed the ranking between the top-1 and top-2 entries.
             </li>
          </ul>



        </p>
        <div class="flexbox-container features">
          <div class="element">
            <div class="icon">
              <img src="https://img.icons8.com/wired/100/000000/leaderboard.png" />
            </div>
            <p>
              Up-to-date leaderboard based <br />
              on 120+ models
            </p>
          </div>
          <div class="element">
            <div class="icon">
              <img src="https://img.icons8.com/ios-glyphs/80/000000/user-credentials.png" />
            </div>
            <p>
              Unified access to 80+ state-of-the-art <br />robust models via
              Model Zoo
            </p>
          </div>
        </div>
      </div>
      <div class="details">
        <div class="box usage">
          <p>Model Zoo</p>
          <div class="divider">
            <hr />
          </div>
          Check out the
          <a href="https://github.com/RobustBench/robustbench#model-zoo">available models</a>
          and our
          <a href="https://github.com/RobustBench/robustbench#notebooks">Colab tutorials</a>.
          <div class="codeblock">
            <div class="vspace10"></div>
            <div style="background: #ffffff; overflow:auto;width:auto;padding:.2em .6em;">
              <pre style="margin: 0; line-height: 125%"><span style="color: #888888"># !pip install git+https://github.com/RobustBench/robustbench@v0.2.1</span>
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">robustbench.utils</span> <span style="color: #008800; font-weight: bold">import</span> load_model
<span style="color: #888888"># Load a model from the model zoo</span>
model <span style="color: #333333">=</span> load_model(model_name<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;Rebuffi2021Fixing_70_16_cutmix_extra&#39;</span>,
                   dataset<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;cifar10&#39;</span>,
                   threat_model<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;Linf&#39;</span>)

<span style="color: #888888"># Evaluate the Linf robustness of the model using AutoAttack</span>
<span style="color: #008800; font-weight: bold">from</span> <span style="color: #0e84b5; font-weight: bold">robustbench.eval</span> <span style="color: #008800; font-weight: bold">import</span> benchmark
clean_acc, robust_acc <span style="color: #333333">=</span> benchmark(model,
                                  dataset<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;cifar10&#39;</span>,
                                  threat_model<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;Linf&#39;</span>)
</pre>
            </div>


            <!-- HTML generated using hilite.me -->
            <div style="
                background: #ffffff;
                overflow: auto;
                width: auto;
                border: solid gray;
                border-width: 0em 0em 0em 0em;
                padding: 0.2em 0.6em;
              ">

              </pre>
            </div>
          </div>
        </div>
        <div class="box images">
          <p>Analysis</p>
          <div class="divider">
            <hr />
          </div>
          Check out <a href="https://arxiv.org/abs/2010.09670">our paper</a> with a detailed analysis.
          <div>
            <!--          <div class="scroller analysis-images">-->
            <img class="analysis" src="./images/aa_robustness_vs_venues_Linf.png" alt="robustness_vs_venues" />
            <!--            <img-->
            <!--              src="./images/aa_robustness_vs_standard_Linf.png"-->
            <!--              alt="robustness_vs_clean"-->
            <!--            />-->
          </div>
        </div>
      </div>
      <div class="vspace10"></div>
    </section>


    <div id="leaderboard" class="container button-list">
      <div class="heading">
        <u>
          Available Leaderboards
        </u>
      </div>
      <a class="btn btn-secondary" href="#OpenOOD_Bench_heading">CIFAR-10 (\( \ell_\infty\))</a>
    </div>


    <section class="container" id="OpenOOD_Bench_heading">
      <div class="heading">
        <p>
          Leaderboard:
          <span class="heading-math">CIFAR-10</span>,
          OpenOOD Benchmark
        </p>
      </div>

      <div id="OpenOOD_Bench">
        <table>
          <thead>
              <tr>
                  <th class="rank">Rank</th>
                  <th class="method">Paper</th>
                  <th class="arch">Alias</th>
                  <!-- <th>Additional Description</th> -->
                  <th class="aa">In-Distribution Accuracy</th>
                  <th class="ca">NearOOD AUROC</th>
                  <th class="aa">FarOOD AUROC</th>
                  <th class="extra-data">Outlier Data</th>
                  <th class="venue">Venue</th>
              </tr>
          </thead>
          <tbody>
              
              <tr>
                  <td class = "ranktd">1</td>
                  <td class = "methoddt"><a href="https://proceedings.mlr.press/v162/sun22d.html">Out-of-Distribution Detection with Deep Nearest Neighbors</a></td>
                  <td class = "archtd">KNN</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">90.64 Â± 0.20</td>
                  <td class = "aatd" >92.96 Â± 0.14</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ICML'22</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">2</td>
                  <td class = "methoddt"><a href="https://proceedings.mlr.press/v70/guo17a.html">On calibration of modern neural network</a></td>
                  <td class = "archtd">TempScale</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">88.08 Â± 0.29</td>
                  <td class = "aatd" >90.97 Â± 0.52</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ICML'17</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">3</td>
                  <td class = "methoddt"><a href="https://openreview.net/forum?id=Hkg4TI9xl">A simple baseline for detecting out-of-distribution and misclassified examples</a></td>
                  <td class = "archtd">MSP</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">88.02 Â± 0.24</td>
                  <td class = "aatd" >90.73 Â± 0.43</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ICLR'17</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">4</td>
                  <td class = "methoddt"><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Bendale_Towards_Open_Set_CVPR_2016_paper.pdf">Towards open-set deep networks</a></td>
                  <td class = "archtd">OpenMax</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">87.61 Â± 0.28</td>
                  <td class = "aatd" >89.62 Â± 0.19</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">CVPR'16</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">5</td>
                  <td class = "methoddt"><a href="https://papers.nips.cc/paper/2020/hash/f5496252609c43eb8a3d147ab9b9c006-Abstract.html">Energy-based Out-of-distribution Detection</a></td>
                  <td class = "archtd">EBO</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">87.59 Â± 0.45</td>
                  <td class = "aatd" >91.21 Â± 0.92</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">NeurIPS'20</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">6</td>
                  <td class = "methoddt"><a href="https://proceedings.mlr.press/v162/hendrycks22a.html">Scaling Out-of-Distribution Detection for Real-World Settings</a></td>
                  <td class = "archtd">MLS</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">87.52 Â± 0.46</td>
                  <td class = "aatd" >91.10 Â± 0.89</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ICML'22</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">7</td>
                  <td class = "methoddt"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_ViM_Out-of-Distribution_With_Virtual-Logit_Matching_CVPR_2022_paper.html">ViM: Out-of-Distribution With Virtual-Logit Matching</a></td>
                  <td class = "archtd">VIM</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">87.07 Â± 0.56</td>
                  <td class = "aatd" >92.20 Â± 0.13</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">CVPR'22</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">8</td>
                  <td class = "methoddt"><a href="https://papers.nips.cc/paper/2021/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html">ReAct: Out-of-distribution Detection With Rectified Activations</a></td>
                  <td class = "archtd">ReAct</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">85.40 Â± 2.98</td>
                  <td class = "aatd" >88.12 Â± 3.13</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">NeurIPS'21</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">9</td>
                  <td class = "methoddt"><a href="https://openreview.net/forum?id=H1VGkIxRZ">Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</a></td>
                  <td class = "archtd">ODIN</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">82.91 Â± 1.83</td>
                  <td class = "aatd" >87.96 Â± 0.61</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ICLR'18</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">10</td>
                  <td class = "methoddt"><a href="https://proceedings.mlr.press/v162/hendrycks22a.html">Scaling Out-of-Distribution Detection for Real-World Settings</a></td>
                  <td class = "archtd">KLM</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">79.04 Â± 0.81</td>
                  <td class = "aatd" >82.68 Â± 0.21</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ICML'22</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">11</td>
                  <td class = "methoddt"><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4405_ECCV_2022_paper.php">DICE: Leveraging Sparsification for Out-of-Distribution Detection</a></td>
                  <td class = "archtd">DICE</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">78.17 Â± 0.80</td>
                  <td class = "aatd" >84.23 Â± 1.89</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ECCV'22</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">12</td>
                  <td class = "methoddt"><a href="https://papers.nips.cc/paper/2018/hash/abdeb6f575ac5c6676b747bca8d09cc2-Abstract.html">A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</a></td>
                  <td class = "archtd">MDS</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">60.42 Â± 0.26</td>
                  <td class = "aatd" >73.90 Â± 0.27</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">NeurIPS'18</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">13</td>
                  <td class = "methoddt"><a href="https://proceedings.mlr.press/v119/sastry20a.html">Detecting Out-of-Distribution Examples with Gram Matrices</a></td>
                  <td class = "archtd">Gram</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">58.65 Â± 4.71</td>
                  <td class = "aatd" >71.74 Â± 3.20</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">ICML'20</td>
              </tr>
              
              <tr>
                  <td class = "ranktd">14</td>
                  <td class = "methoddt"><a href="https://papers.nips.cc/paper/2021/hash/063e26c670d07bb7c4d30e6fc69fe056-Abstract.html">On the Importance of Gradients for Detecting Distributional Shifts in the Wild</a></td>
                  <td class = "archtd">GradNorm</td>
                  <!-- <td>nan</td> -->
                  <td class = "aatd">95.06 Â± 0.30</td>
                  <td class = "catd">55.04 Â± 0.93</td>
                  <td class = "aatd" >57.55 Â± 3.22</td>
                  <td class = "datatd">No</td>
                  <td class = "venuetd">NeurIPS'21</td>
              </tr>
              
          </tbody>
      </table>
      <script>
          $(document).ready(function () {
              $("#_leaderboard_").DataTable({
                  lengthMenu: [15, 25, 50, 75, 100],
                  "drawCallback": function (settings) {
                      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
                  },
                  language: {
                      searchPlaceholder: "Papers, architectures, venues"
                  },
                  
                  columnDefs: [
                      { width: "15%", targets: 4 },
                      { width: "15%", targets: 5 }
                  ]
                  
              });
          });
      </script>
      </div>
    </section>

    <div class="vspace30"></div>

    <section id="faq">
      <div class="heading">
        <p>FAQ</p>
      </div>

      <p class="qa-box">
        <span class="question">&#10148; How does the RobustBench leaderboard differ from the
          <a href="https://github.com/fra31/auto-attack">AutoAttack leaderboard</a>? ðŸ¤”
        </span>
        <br />
        <span class="answer"> The <a href="https://github.com/fra31/auto-attack">AutoAttack leaderboard</a> was
          the starting point of RobustBench. Now only the RobustBench leaderboard is actively maintained.
        </span>
      </p>

      <p class="qa-box">
        <span class="question">&#10148; How does the RobustBench leaderboard differ from
          <a href="https://www.robust-ml.org/">robust-ml.org</a>? ðŸ¤”
        </span>
        <br />
        <span class="answer"><a href="https://www.robust-ml.org/">robust-ml.org</a> focuses on
          <em>adaptive</em> evaluations, but we provide a
          <strong>standardized benchmark</strong>. Adaptive evaluations have been very useful (e.g., see
          <a href="https://arxiv.org/abs/2002.08347">Tramer et al., 2020</a>),
          but they are also very time-consuming and cannot be standardized by definition. Instead, we argue
          that one can estimate robustness accurately mostly <em>without</em> adaptive
          attacks but for this one has to introduce some restrictions on the
          considered models (see <a href="https://arxiv.org/abs/2010.09670">our paper</a> for more details).
          However, we do welcome adaptive evaluations and we are always interested in showing the best known
          robust accuracy.
        </span>
      </p>

      <p class="qa-box">
        <span class="question">&#10148; How is it related to libraries like
          <a href="https://github.com/bethgelab/foolbox">foolbox</a> /
          <a href="https://github.com/tensorflow/cleverhans">cleverhans</a> /
          <a href="https://github.com/BorealisAI/advertorch">advertorch</a>? ðŸ¤”
        </span>
        <br />
        <span class="answer">These libraries provide implementations of different
          <em>attacks</em>. Besides the standardized benchmark,
          <strong>RobustBench</strong> additionally provides a repository of the
          most robust models. So you can start using the robust models in one
          line of code (see the tutorial
          <a href="https://github.com/RobustBench/robustbench#model-zoo-quick-tour">here</a>).</span>
      </p>

      <p class="qa-box">
        <span class="question">&#10148; Why is Lp-robustness still interesting in 2021? ðŸ¤”
        </span>
        <br />
        <span class="answer">There are numerous interesting applications of Lp-robustness that
          span transfer learning (<a href="https://arxiv.org/abs/2007.08489">Salman et al. (2020)</a>,
          <a href="https://arxiv.org/abs/2007.05869">Utrera et al. (2020)</a>),
          interpretability (<a href="https://arxiv.org/abs/1805.12152">Tsipras et al. (2018)</a>, <a
            href="https://arxiv.org/abs/1910.08640">Kaur et al. (2019)</a>,
          <a href="https://arxiv.org/abs/1906.00945">Engstrom et al. (2019)</a>), security (<a
            href="https://arxiv.org/abs/1811.03194">TramÃ¨r et al. (2018)</a>,
          <a href="https://arxiv.org/abs/1906.07153">Saadatpanah et al. (2019)</a>), generalization (<a
            href="https://arxiv.org/abs/1911.09665">Xie et al. (2019)</a>, <a
            href="https://arxiv.org/abs/1909.11764">Zhu et al. (2019)</a>,
          <a href="https://arxiv.org/abs/2004.10934">Bochkovskiy et al. (2020)</a>), robustness to unseen perturbations
          (<a href="https://arxiv.org/abs/1911.09665">Xie et al. (2019)</a>, <a
            href="https://arxiv.org/abs/1905.01034">Kang et al. (2019)</a>),
          stabilization of GAN training (<a href="https://arxiv.org/abs/2008.03364">Zhong et al. (2020)</a>).</span>
      </p>

      <p class="qa-box">
        <span class="question">&#10148; What about verified adversarial robustness? ðŸ¤”
        </span>
        <br />
        <span class="answer">We mostly focus on defenses which improve
          <em>empirical</em> robustness, given the lack of clarity regarding
          which approaches really improve robustness and which only make some
          particular attacks unsuccessful.
          However, we do not restrict submissions of verifiably robust models (e.g., we have
          <a href="https://arxiv.org/abs/1906.06316">Zhang et al. (2019)</a> in our CIFAR-10 Linf leaderboard).
          For methods targeting verified robustness, we encourage the readers to check out
          <a href="https://arxiv.org/abs/1902.08722">Salman et al. (2019)</a>
          and <a href="https://arxiv.org/abs/2009.04131">Li et al. (2020)</a>.
        </span>
      </p>

      <p class="qa-box">
        <span class="question">&#10148; What if I have a better attack than the one used in this
          benchmark? ðŸ¤”
        </span>
        <br />
        <span class="answer">We will be happy to add a better attack or any adaptive evaluation
          that would complement our default standardized attacks!</span>
      </p>
    </section>



    <div class="vspace50"></div>

    <section id="citation">
      <div class="heading">
        <p>Citation</p>
      </div>
      Consider citing our whitepaper if you want to reference our leaderboard or if you are using the models from the
      Model Zoo:
      <!--      @article{croce2020robustbench,-->
      <!--        title={RobustBench: a standardized adversarial robustness benchmark},-->
      <!--        author={Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Matthias Hein},-->
      <!--        journal={arXiv preprint arXiv:2010.09670},-->
      <!--        year={2020}-->
      <!--      }-->
      <!-- HTML generated using hilite.me -->
      <div
        style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.0em .0em .0em .0em;padding:.2em .6em;">
        <pre style="margin: 0; line-height: 125%"><span style="color: #555555; font-weight: bold">@article</span>{croce2020robustbench,
    title<span style="color: #333333">=</span>{RobustBench: a standardized adversarial robustness benchmark},
    author<span style="color: #333333">=</span>{Croce, Francesco <span style="color: #000000; font-weight: bold">and</span> Andriushchenko, Maksym <span style="color: #000000; font-weight: bold">and</span> Sehwag, Vikash <span style="color: #000000; font-weight: bold">and</span> Debenedetti, Edoardo <span style="color: #000000; font-weight: bold">and</span> Flammarion, Nicolas
    <span style="color: #000000; font-weight: bold">and</span> Chiang, Mung <span style="color: #000000; font-weight: bold">and</span> Mittal, Prateek <span style="color: #000000; font-weight: bold">and</span> Matthias Hein},
    journal<span style="color: #333333">=</span>{arXiv preprint arXiv:2010.09670},
    year<span style="color: #333333">=</span>{2020}
}</pre>
      </div>

    </section>

    <div class="vspace50"></div>

    <section id="contribute">
      <div class="details">
        <div class="box2">
          <p>Contribute to RobustBench!</p>
          <div class="divider">
            <hr />
          </div>
          We welcome any contribution in terms of both new robust models and
          evaluations. Please check
          <a href="https://github.com/RobustBench/robustbench#how-to-contribute">here</a>
          for more details.
          <br />
          <br />
          Feel free to contact us at
          <a href="mailto:adversarial.benchmark@gmail.com">adversarial.benchmark@gmail.com</a>
        </div>
        <div class="box2">
          <p>Maintainers</p>
          <div class="divider">
            <hr />
          </div>
          <ul>
            <li>
              <a href="https://twitter.com/fra__31" target="_blank">Francesco Croce
              </a>
              <a href="https://twitter.com/fra__31"><i class="fas fa-globe"></i></a>
              <a href="https://github.com/fra31"><i class="fab fa-github"></i></a>
              <a href="https://scholar.google.com/citations?user=laq9cq0AAAAJ"><i class="ai ai-google-scholar"></i></a>
            </li>
            <li>
              <a href="https://people.epfl.ch/maksym.andriushchenko" target="_blank">Maksym Andriushchenko</a>
              <a href="https://people.epfl.ch/maksym.andriushchenko"><i class="fas fa-globe"></i></a>
              <a href="https://github.com/max-andr"><i class="fab fa-github"></i></a>
              <a href="https://scholar.google.com/citations?user=ZNtuJYoAAAAJ"><i class="ai ai-google-scholar"></i></a>
            </li>
            <li>
              <a href="https://vsehwag.github.io/" target="_blank">Vikash Sehwag</a>
              <a href="https://vsehwag.github.io/"><i class="fas fa-globe"></i></a>
              <a href="https://github.com/VSehwag"><i class="fab fa-github"></i></a>
              <a href="https://scholar.google.com/citations?user=JAkeEG8AAAAJ"><i class="ai ai-google-scholar"></i></a>
            </li>
            <li>
              <a href="https://edoardo.science" target="_blank">Edoardo Debenedetti</a>
              <a href="https://edoardo.science"><i class="fas fa-globe"></i></a>
              <a href="https://github.com/dedeswim"><i class="fab fa-github"></i></a>
              <a href="https://twitter.com/edoardo_debe"><i class="fab fa-twitter"></i></a>
            </li>
          </ul>
        </div>
      </div>
    </section>
  </div>

  <hr class="bottomrule" />

  <footer>
    <small>&copy; 2021, RobustBench;
      <a href="https://icons8.com/icon/100413/access">Icons from Icons8</a></small>
  </footer>

  <script>
    // When the user scrolls the page, execute myFunction
    window.onscroll = function () {
      myFunction();
    };
    // Get the navbar
    var navbar = document.getElementById("navbar");
    // Get the offset position of the navbar
    var sticky = navbar.offsetTop;
    // Add the sticky class to the navbar when you reach its scroll position. Remove "sticky" when you leave the scroll position
    function myFunction() {
      if (window.pageYOffset >= sticky) {
        navbar.classList.add("sticky");
      } else {
        navbar.classList.remove("sticky");
      }
    }
  </script>
  <script>
    $("#OpenOOD_Bench").load("/home/hz271/Research/OpenOOD.github.io/output/CIFAR10.html");
  </script>
</body>
